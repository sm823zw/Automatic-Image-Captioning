{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import torch\r\n",
    "import nltk\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision.transforms as transforms\r\n",
    "from load_data import get_loader\r\n",
    "from model import EncoderDecoder\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "transform = transforms.Compose([\r\n",
    "    transforms.Resize(226),                     \r\n",
    "    transforms.RandomCrop(224),                 \r\n",
    "    transforms.ToTensor(),                               \r\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\r\n",
    "])\r\n",
    "\r\n",
    "train_loader, test_loader, train_dataset, test_dataset = get_loader(\r\n",
    "    root_folder=\"flickr8k/images\",\r\n",
    "    train_annotation_file=\"flickr8k/train_captions.txt\",\r\n",
    "    test_annotation_file=\"flickr8k/test_captions.txt\",\r\n",
    "    transform=transform,\r\n",
    "    num_workers=2\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "len(train_loader), len(test_loader), len(train_dataset), len(test_dataset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1012, 253, 32364, 8091)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "torch.backends.cudnn.benchmark = True\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "\r\n",
    "# Hyperparameters\r\n",
    "embed_size=300\r\n",
    "vocab_size = len(train_dataset.vocab)\r\n",
    "attention_dim=256\r\n",
    "encoder_dim=2048\r\n",
    "decoder_dim=512\r\n",
    "learning_rate = 3e-4\r\n",
    "\r\n",
    "\r\n",
    "# initialize model, loss etc\r\n",
    "model = EncoderDecoder(embed_size, vocab_size, attention_dim, encoder_dim, decoder_dim, drop_prob=0.3).to(device)\r\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def test_function(model, test_loader):\r\n",
    "    model.eval()\r\n",
    "    bleu_score = 0\r\n",
    "    # f = open(\"results.txt\", \"w\")\r\n",
    "    for idx, (images, correct_captions) in enumerate(iter(test_loader)):\r\n",
    "        features = model.encoder(images.to(device))\r\n",
    "        for i in range(features.shape[0]):\r\n",
    "            caps, alphas = model.decoder.generate_caption(features[i:i+1], vocab=train_dataset.vocab)\r\n",
    "            caps = caps[:-1]\r\n",
    "            caption = ' '.join(caps)\r\n",
    "            correct_caption = []\r\n",
    "            for j in correct_captions[i:i+1][0]:\r\n",
    "                if j.item() not in [0, 1, 2]:\r\n",
    "                    correct_caption.append(test_dataset.vocab.itos[j.item()])\r\n",
    "            bleu = nltk.translate.bleu_score.sentence_bleu([correct_caption], caps, weights=(0.5, 0.5))\r\n",
    "            correct_caption = ' '.join(correct_caption)\r\n",
    "            write_this = correct_caption + ', ' + caption + '\\n'\r\n",
    "            bleu_score += bleu\r\n",
    "            # f.write(write_this)\r\n",
    "    # f.close()\r\n",
    "    return bleu_score/8091\r\n",
    "\r\n",
    "\r\n",
    "def save_model(model,num_epochs):\r\n",
    "    model_state = {\r\n",
    "        'num_epochs':num_epochs,\r\n",
    "        'embed_size':embed_size,\r\n",
    "        'vocab_size':len(train_dataset.vocab),\r\n",
    "        'attention_dim':attention_dim,\r\n",
    "        'encoder_dim':encoder_dim,\r\n",
    "        'decoder_dim':decoder_dim,\r\n",
    "        'state_dict':model.state_dict()\r\n",
    "    }\r\n",
    "\r\n",
    "    torch.save(model_state,'attention_model_state.pth')\r\n",
    "\r\n",
    "checkpoint = torch.load('attention_model_state.pth')\r\n",
    "model.load_state_dict(checkpoint['state_dict'])\r\n",
    "epoch = checkpoint['num_epochs']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "num_epochs = 5\r\n",
    "print_every = 100\r\n",
    "for epoch in range(1, num_epochs + 1):\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    for idx, (image, captions) in enumerate(iter(train_loader)):\r\n",
    "        image, captions = image.to(device), captions.to(device)\r\n",
    "        # Zero the gradients.\r\n",
    "        optimizer.zero_grad()\r\n",
    "        # Feed forward\r\n",
    "        outputs, attentions = model(image, captions)\r\n",
    "        # Calculate the batch loss.\r\n",
    "        targets = captions[:,1:]\r\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\r\n",
    "        # Backward pass.\r\n",
    "        loss.backward()\r\n",
    "        # Update the parameters in the optimizer.\r\n",
    "        optimizer.step()\r\n",
    "        if (idx+1)%print_every == 0:\r\n",
    "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\r\n",
    "    \r\n",
    "    print(test_function(model, test_loader))\r\n",
    "\r\n",
    "    #save the latest model\r\n",
    "    save_model(model, epoch)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 loss: 1.34103\n",
      "Epoch: 1 loss: 1.47104\n",
      "Epoch: 1 loss: 1.51540\n",
      "Epoch: 1 loss: 1.43858\n",
      "Epoch: 1 loss: 1.52330\n",
      "Epoch: 1 loss: 1.63536\n",
      "Epoch: 1 loss: 1.54243\n",
      "Epoch: 1 loss: 1.63800\n",
      "Epoch: 1 loss: 1.61689\n",
      "Epoch: 1 loss: 1.70182\n",
      "0.1738885610679138\n",
      "Epoch: 2 loss: 1.54467\n",
      "Epoch: 2 loss: 1.45809\n",
      "Epoch: 2 loss: 1.34410\n",
      "Epoch: 2 loss: 1.50166\n",
      "Epoch: 2 loss: 1.55057\n",
      "Epoch: 2 loss: 1.53857\n",
      "Epoch: 2 loss: 1.62845\n",
      "Epoch: 2 loss: 1.65814\n",
      "Epoch: 2 loss: 1.63853\n",
      "Epoch: 2 loss: 1.50285\n",
      "0.17804776382807722\n",
      "Epoch: 3 loss: 1.54221\n",
      "Epoch: 3 loss: 1.34699\n",
      "Epoch: 3 loss: 1.59455\n",
      "Epoch: 3 loss: 1.64221\n",
      "Epoch: 3 loss: 1.47994\n",
      "Epoch: 3 loss: 1.60703\n",
      "Epoch: 3 loss: 1.50478\n",
      "Epoch: 3 loss: 1.68979\n",
      "Epoch: 3 loss: 1.49244\n",
      "Epoch: 3 loss: 1.45444\n",
      "0.17801017608868377\n",
      "Epoch: 4 loss: 1.70248\n",
      "Epoch: 4 loss: 1.42919\n",
      "Epoch: 4 loss: 1.53869\n",
      "Epoch: 4 loss: 1.47361\n",
      "Epoch: 4 loss: 1.62460\n",
      "Epoch: 4 loss: 1.33672\n",
      "Epoch: 4 loss: 1.35910\n",
      "Epoch: 4 loss: 1.41541\n",
      "Epoch: 4 loss: 1.61562\n",
      "Epoch: 4 loss: 1.26492\n",
      "0.177392239158814\n",
      "Epoch: 5 loss: 1.34407\n",
      "Epoch: 5 loss: 1.47319\n",
      "Epoch: 5 loss: 1.41713\n",
      "Epoch: 5 loss: 1.40618\n",
      "Epoch: 5 loss: 1.28952\n",
      "Epoch: 5 loss: 1.60325\n",
      "Epoch: 5 loss: 1.52909\n",
      "Epoch: 5 loss: 1.48772\n",
      "Epoch: 5 loss: 1.38367\n",
      "Epoch: 5 loss: 1.56952\n",
      "0.17767874690788316\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model.eval()\r\n",
    "bleu_score = 0\r\n",
    "f = open(\"results.txt\", \"w\")\r\n",
    "for idx, (images, correct_captions) in enumerate(iter(test_loader)):\r\n",
    "    features = model.encoder(images.to(device))\r\n",
    "    for i in range(features.shape[0]):\r\n",
    "        caps, alphas = model.decoder.generate_caption(features[i:i+1], vocab=train_dataset.vocab)\r\n",
    "        caps = caps[:-1]\r\n",
    "        caption = ' '.join(caps)\r\n",
    "        correct_caption = []\r\n",
    "        for j in correct_captions[i:i+1][0]:\r\n",
    "            if j.item() not in [0, 1, 2]:\r\n",
    "                correct_caption.append(test_dataset.vocab.itos[j.item()])\r\n",
    "        bleu = nltk.translate.bleu_score.sentence_bleu([correct_caption], caps, weights=(0.5, 0.5))\r\n",
    "        correct_caption = ' '.join(correct_caption)\r\n",
    "        write_this = correct_caption + ', ' + caption + '\\n'\r\n",
    "        bleu_score += bleu\r\n",
    "        f.write(write_this)\r\n",
    "f.close()\r\n",
    "print(bleu_score/8091)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.1763335136512202\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec7d137c6e0bfe5aec1849c5c512dd0bf44cf2eb2fae9fc2de49724729b3d6c6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('torch1.7': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}